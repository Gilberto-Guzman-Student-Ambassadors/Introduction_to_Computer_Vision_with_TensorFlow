{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural networks\n",
        "\n",
        "In the previous unit we learned how to define a generic multi-layered neural network. In this unit we will learn about **Convolutional Neural Networks (CNNs)**, which are specifically designed for computer vision.\n",
        "\n",
        "Computer vision is different from generic classification, because when we are trying to find a certain object in the picture, we are scanning the image looking for some specific **patterns** and their combinations. For example, when looking for a cat, we first may look for horizontal lines, which can form whiskers, and then certain combination of whiskers can tell us that it is actually a picture of a cat. The position and presence of certain patterns are important. \n",
        "\n",
        "To extract patterns, we will use the notion of **convolutional filters**. But first, let us load all dependencies and functions that we have defined in the previous units. We will also import `tfcv`, a helper library that contain some useful functions that we do not want to define inside this notebook to keep the code short and clean. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
        "# we will set tensorflow option to grow GPU memory allocation when required.\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "if len(physical_devices)>0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-09-06 16:21:47.745931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-06 16:21:47.770049: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-06 16:21:47.778014: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-09-06 16:21:47.796099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1725639721734
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1725639721872
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-fetch the data when running in sandbox environment\n",
        "!mkdir -p ~/.keras/datasets\n",
        "!wget -P ~/.keras/datasets -q https://github.com/MicrosoftDocs/tensorflowfundamentals/raw/main/data/mnist.npz\n",
        "# Load tfcv.py module\n",
        "!wget -O tfcv.py -q https://github.com/MicrosoftDocs/tensorflowfundamentals/raw/main/computer-vision-tf/tfcv.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639721997
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfcv import *\n",
        "(x_train,y_train),(x_test,y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722112
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional filters\n",
        "\n",
        "Convolutional filters are small windows that run over each pixel of the image and compute weighted average of the neighboring pixels.\n",
        "\n",
        "\n",
        "\n",
        "They are defined by matrices of weight coefficients. Let's see the examples of applying two different convolutional filters over our MNIST handwritten digits:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plot_convolution(x_train[:5],[[-1.,0.,1.],[-1.,0.,1.],[-1.,0.,1.]],'Vertical edge filter')\n",
        "plot_convolution(x_train[:5],[[-1.,-1.,-1.],[0.,0.,0.],[1.,1.,1.]],'Horizontal edge filter')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722230
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First filter is called a **vertical edge filter**, and it is defined by the following matrix:\n",
        "$$\n",
        "\\left(\n",
        "    \\begin{matrix}\n",
        "     -1 & 0 & 1 \\cr\n",
        "     -1 & 0 & 1 \\cr\n",
        "     -1 & 0 & 1 \\cr\n",
        "    \\end{matrix}\n",
        "\\right)\n",
        "$$\n",
        "When this filter goes over relatively uniform pixel field, all values add up to 0. When it encounters a vertical edge in the image, high spiked value is generated. That's why in the images above you can see vertical edges represented by high and low values, while horizontal edges are averaged out.\n",
        "\n",
        "An opposite thing happens when we apply horizontal edge filter - horizontal lines are amplified, and vertical are averaged out.\n",
        "\n",
        "In classical computer vision, multiple filters were applied to the image to generate features, which then were used by machine learning algorithm to build a classifier. In deep learning we construct networks that **learn** the best convolutional filters to solve classification problem on its own.\n",
        "\n",
        "To do that, we introduce **convolutional layers**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional layers\n",
        "\n",
        "Convolutional layers are defined using `Conv2d` class. We need to specify the following:\n",
        "* `filters` - number of filters to use. We will use 9 different filters, which will give the network plenty of opportunities to explore which filters work best for our scenario.\n",
        "* `kernel_size` is the size of the sliding window. Usually 3x3 or 5x5 filters are used.\n",
        "\n",
        "The simplest CNN will contain only one convolutional layer. Given the input size 28x28, after applying nine 5x5 filters we will end up with a tensor of 24x24x9. The spatial dimension is smaller, because there are only 24 positions where a sliding interval of length 5 can fit into 28 pixels.\n",
        "\n",
        "After the convolution layer, we flatten 24x24x9 tensor into one vector size of 5184. Then add the linear layer to produce 10 classes and use `relu` activation function in between layers. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=9, kernel_size=(5,5), input_shape=(28,28,1),activation='relu'),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722354
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that this network contains around 50k trainable parameters, compared to around 80k in fully-connected multi-layered networks. Convolutional networks generalize better which allows us to achieve good results on smaller datasets.\n",
        "\n",
        "> **Note**: In most of the practical cases, we want to apply convolutional layers to color images. Thus, `Conv2D` layer expects the input to be of the shape $W\\times H\\times C$, where $W$ and $H$ are width and height of the image, and $C$ is the number of color channels. For grayscale images, we need the same shape with $C=1$.\n",
        "\n",
        "We need to reshape our data before starting training:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_c = np.expand_dims(x_train,3)\n",
        "x_test_c = np.expand_dims(x_test,3)\n",
        "hist = model.fit(x_train_c,y_train,validation_data=(x_test_c,y_test),epochs=3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722473
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(hist)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722592
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we are able to achieve higher accuracy with fewer epochs compared to the fully-connected networks from the previous unit. However, the training itself requires more resources, and may be slower on non-GPU computers.\n",
        "\n",
        "## Visualizing Convolutional Layers\n",
        "\n",
        "We can also visualize the weights of our trained convolutional layers to try and make some more sense of what is going on:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,9)\n",
        "l = model.layers[0].weights[0]\n",
        "for i in range(9):\n",
        "    ax[i].imshow(l[...,0,i])\n",
        "    ax[i].axis('off')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722718
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that some of those filters look like they can recognize some oblique strokes, while others look pretty random. \n",
        "\n",
        "> **Task**: Train the same network with 3x3 filters and visualize them. Do you see more familiar patterns?\n",
        "\n",
        "## Multi-layered CNNs and pooling layers\n",
        "\n",
        "First the convolutional layers looks for primitive patterns, such as horizontal or vertical lines. We can apply further convolutional layers on top of them to look for higher-level patterns, such as primitive shapes. Then more convolutional layers can combine those shapes into some parts of the picture, up to the final object that we are trying to classify. \n",
        "\n",
        "When doing so, we may also apply one trick: reducing the spatial size of the image. Once we have detected there is a horizontal stoke within sliding 3x3 window, it is not so important at which exact pixel it occurred. Thus we can \"scale down\" the size of the image, which is done using one of the **pooling layers**:\n",
        "\n",
        " * **Average Pooling** takes a sliding window (for example, 2x2 pixels) and computes an average of values within the window\n",
        " * **Max Pooling** replaces the window with the maximum value. The idea behind max pooling is to detect a presence of a certain pattern within the sliding window.\n",
        "\n",
        "Thus, in a typical CNN there would be several convolutional layers, with pooling layers in between them to decrease dimensions of the image. We would also increase the number of filters, because as patterns become more advanced - there are more possible interesting combinations that we need to be looking for.\n",
        "\n",
        "![An image showing several convolutional layers with pooling layers.](notebooks/images/cnn-pyramid.png)\n",
        "\n",
        "This architecture is also called **pyramid architecture** because of decreasing spatial dimensions and increasing feature/filters dimensions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=10, kernel_size=(5,5), input_shape=(28,28,1),activation='relu'),\n",
        "    keras.layers.MaxPooling2D(),\n",
        "    keras.layers.Conv2D(filters=20, kernel_size=(5,5), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(),    \n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722837
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the number of trainable parameters (~8.5K) is dramatically smaller than in the previous cases. This happens because convolutional layers in general have few parameters, and dimensionality of the image before applying final dense layer is significantly reduced."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x_train_c,y_train,validation_data=(x_test_c,y_test),epochs=3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639722960
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(hist)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723087
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that we are able to achieve a higher accuracy with more than one layer, and it need a smaller number of epochs. It means that a more sophisticated network architecture less data to figure out what is going on, and to extract generic patterns from our images. However, training also takes longer and requires a GPU.\n",
        "\n",
        "## Playing with real images from the CIFAR-10 dataset\n",
        "\n",
        "While our handwritten digit recognition problem may seem like a toy problem, we are now ready to do something more serious. Let's explore a more advanced dataset of pictures of different objects, called [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html). It contains 60k 32x32 images, divided into 10 classes. \n",
        "\n",
        "> When running in sandbox, we need to execute the following cell to pre-fetch CIFAR-10 dataset. You can skip the following cell if you are running the code from your own notebook."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-fetch CIFAR-10 dataset when running in sandbox\n",
        "!wget -P ~/.keras/datasets -q https://mslearntensorflowlp.blob.core.windows.net/data/cifar-10-batches-py.tar.gz"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723206
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_dataset(x_train,y_train,classes=classes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723445
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A well-known architecture for CIFAR-10 is called [LeNet](https://en.wikipedia.org/wiki/LeNet), and has been proposed by *Yann LeCun*. It follows the same principles as we have outlined above, the main difference being 3 input color channels instead of 1. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters = 6, kernel_size = 5, strides = 1, activation = 'relu', input_shape = (32,32,3)),\n",
        "    keras.layers.MaxPooling2D(pool_size = 2, strides = 2),\n",
        "    keras.layers.Conv2D(filters = 16, kernel_size = 5, strides = 1, activation = 'relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size = 2, strides = 2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(120, activation = 'relu'),\n",
        "    keras.layers.Dense(84, activation = 'relu'),\n",
        "    keras.layers.Dense(10, activation = 'softmax')])\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723566
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training this network properly will a take significant amount of time and should be done on GPU-enabled compute."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc'])\n",
        "hist = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723689
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(hist)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1725639723805
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy that we have been able to achieve with only a few epochs of training is just ok. Remember that our problem is actually significantly more difficult than MNIST digit classification. Getting above 60% accuracy is a good accomplishment in such a short training time.\n",
        "\n",
        "## Takeaways\n",
        "\n",
        "In this unit, we have learned the main concept behind computer vision neural networks - convolutional networks. Real-life architectures that power image classification, object detection, and even image generation networks are all based on CNNs, just with more layers and some additional training tricks."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "86193a1ab0ba47eac1c69c1756090baa3b420b3eea7d4aafab8b85f8b312f0c5"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}